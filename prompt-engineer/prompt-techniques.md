# Prompting techniques
Prompting techniques help to improve the accuracy of the text generated by LLMs by providing a structured framework or guidelines.


## The concept of a Shot
SHOT in prompt engineering is all about the number of examples given to a machine learning mode

## Zero-Shot
Zero-shot prompting is a technique that involves generating text from an LLM without additional training or fine-tuning. It is achieved by providing a prompt that includes the task description and the desired output. The LLM then generates the output based on the prompt and its pre-trained knowledge.

## Few-Shot
Few-shot prompting involves training LLMs on a few training examples to perform specific tasks. Unlike zero-shot prompt generation, few-shot prompting allows for the customization of LLMs to suit specific task requirements. Few-shot models use a few examples to adapt to a specific type of task, making it a useful method for customizing text generation pipelines.

## Chain-of-Thought

An advanced version of few-shot prompting, Chain-of-Thought prompting trains models to identify and understand the connections between various text aspects. The models are educated to detect these connections and utilize this information to generate appropriate responses. With immense utility in applications like question-answering and summarization, this technique demands an in-depth appreciation of the relationships within text pieces.

**You can also attempt to activate the Chain-of-Thought mode in LLM by including the instruction "let's think step-by-step" into your prompt.**

## Three of thoughts

The concept of ToT involves requesting the model to present various alternatives instead of an immediate solution. Once you obtain these alternatives, you assess them and encourage LLM to expand on the most favorable ones while disregarding the less desirable ones. You continue this process until you achieve the best outcome for your requirements.